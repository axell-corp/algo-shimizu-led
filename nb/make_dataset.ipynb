{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([987, 9, 320, 436, 3])\n",
      "torch.Size([987, 9, 128])\n",
      "torch.Size([987, 8, 267])\n",
      "torch.Size([987, 267])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "\n",
    "n = 7\n",
    "data_id = 1\n",
    "n_time = 9\n",
    "frame_max = 150\n",
    "n_sample = frame_max - n_time\n",
    "\n",
    "led_data = \"../data/dataset/led/led_{}.csv\".format(data_id)\n",
    "lcd_data = \"../data/dataset/lcd/lcd_{}.mp4\".format(data_id)\n",
    "spec_data = \"../data/dataset/spc/spec_{}.csv\".format(data_id)\n",
    "led = np.loadtxt(led_data, dtype=np.int32)\n",
    "spec = np.loadtxt(spec_data, dtype=np.float32)\n",
    "lcd = cv2.VideoCapture(lcd_data)\n",
    "lcd_shape = (int(lcd.get(cv2.CAP_PROP_FRAME_WIDTH)), int(lcd.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "lcd_data = torch.zeros((n, frame_max, lcd_shape[1], lcd_shape[0], 3), dtype=torch.uint8)\n",
    "spec_data = torch.zeros((n, frame_max, spec.shape[0]))\n",
    "led_data = torch.zeros((n, frame_max, led.shape[0]))\n",
    "lcd_dataset = torch.zeros((n_sample * n, n_time, lcd_shape[1], lcd_shape[0], 3))\n",
    "spec_dataset = torch.zeros((n_sample * n, n_time, spec.shape[0]))\n",
    "led_dataset = torch.zeros((n_sample * n, n_time - 1, led.shape[0]))\n",
    "led_correct = torch.zeros(n_sample * n, led.shape[0])\n",
    "\n",
    "print(lcd_dataset.shape)\n",
    "print(spec_dataset.shape)\n",
    "print(led_dataset.shape)\n",
    "print(led_correct.shape)\n",
    "\n",
    "for data_id in range(1, n + 1):\n",
    "    led_file = \"../data/dataset/led/led_{}.csv\".format(data_id)\n",
    "    lcd_file = \"../data/dataset/lcd/lcd_{}.mp4\".format(data_id)\n",
    "    spec_file = \"../data/dataset/spc/spec_{}.csv\".format(data_id)\n",
    "    led = np.loadtxt(led_file, dtype=np.int32)\n",
    "    spec = np.loadtxt(spec_file, dtype=np.float32)\n",
    "    lcd = cv2.VideoCapture(lcd_file)\n",
    "    frame_len = int(lcd.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    lcd_shape = (int(lcd.get(cv2.CAP_PROP_FRAME_WIDTH)), int(lcd.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    lcd_vec = np.zeros((frame_len, lcd_shape[1], lcd_shape[0], 3), dtype=np.uint8)\n",
    "    for frame_idx in range(frame_len):\n",
    "        lcd.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        e, frame = lcd.read()\n",
    "        if not e:\n",
    "            print(\"err\")\n",
    "        lcd_vec[frame_idx, :, :, :] = frame\n",
    "\n",
    "    lcd_data[data_id - 1, 0:frame_len, :, :, :] = torch.tensor(lcd_vec)\n",
    "    spec_data[data_id - 1, 0:frame_len:, :] = torch.tensor(spec.T)\n",
    "    led_data[data_id - 1, 0:frame_len, :] = torch.tensor(led.T)\n",
    "\n",
    "    for data_id in range(n):\n",
    "        for sample_id in range(n_sample):\n",
    "            outer_idx = n_sample * data_id + sample_id\n",
    "            lcd_dataset[outer_idx] = lcd_data[data_id, sample_id:sample_id + n_time, :, :, :]\n",
    "            led_dataset[outer_idx] = led_data[data_id, sample_id:sample_id + n_time - 1, :]\n",
    "            spec_dataset[outer_idx] = spec_data[data_id, sample_id:sample_id + n_time, :]\n",
    "            led_correct[outer_idx] = led_data[data_id, sample_id + n_time, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 267])\n",
      "torch.Size([1, 583])\n",
      "torch.Size([1, 133])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "c1 = nn.Conv1d(n_time - 1, 1, 3, 2)\n",
    "c11 = nn.Conv1d(n_time, 1, 3, 1)\n",
    "c2 = nn.Conv3d(n_time, 1, (3, 3, 1), (2, 2, 1)) \n",
    "c3 = nn.Conv3d(1, 1, (3, 3, 1), (2, 2, 1))\n",
    "c4 = nn.Conv3d(1, 1, (3, 3, 1), (2, 2, 1))\n",
    "y0 = c1(led_dataset[3:4])\n",
    "print(led_dataset[3:4].shape)\n",
    "y1 = c11(spec_dataset[3:4])\n",
    "y2 = c2(lcd_dataset[10:11])\n",
    "y3 = c3(y2)\n",
    "y4 = c4(y3)\n",
    "y5 = c4(y4)\n",
    "y6 = c4(y5)\n",
    "y7 = torch.reshape(y0, y0.shape[:1] + (-1,))\n",
    "y8 = torch.reshape(y1, y1.shape[:1] + (-1,))\n",
    "y9 = torch.reshape(y6, y6.shape[:1] + (-1,))\n",
    "y10 = torch.cat([y7, y8, y9], 1)\n",
    "print(y10.shape)\n",
    "lstm = nn.LSTM(input_size=583, hidden_size=64, batch_first=True)\n",
    "y11, h = lstm(y10)\n",
    "y12 = nn.Linear(64, 133)(y11)\n",
    "print(y12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "dataset = TensorDataset(lcd_dataset, spec_dataset, led_dataset, led_correct)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (spec_conv): Conv1d(9, 9, kernel_size=(5,), stride=(1,))\n",
      "  (c1_1): Conv1d(8, 1, kernel_size=(3,), stride=(2,))\n",
      "  (c1_2): Conv1d(9, 1, kernel_size=(3,), stride=(1,))\n",
      "  (c2): Conv3d(9, 4, kernel_size=(3, 3, 1), stride=(2, 2, 1))\n",
      "  (c3): Conv3d(4, 1, kernel_size=(3, 3, 1), stride=(2, 2, 1))\n",
      "  (c4): Conv3d(1, 1, kernel_size=(3, 3, 1), stride=(2, 2, 1))\n",
      "  (lstm): LSTM(583, 64, batch_first=True)\n",
      "  (lin): Linear(in_features=64, out_features=267, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.spec_conv = nn.Conv1d(\n",
    "            n_time, n_time, 5\n",
    "        )\n",
    "        self.c1_1 = nn.Conv1d(n_time - 1, 1, 3, 2)\n",
    "        self.c1_2 = nn.Conv1d(n_time, 1, 3, 1)\n",
    "        self.c2 = nn.Conv3d(n_time, 4, (3, 3, 1), (2, 2, 1)) \n",
    "        self.c3 = nn.Conv3d(4, 1, (3, 3, 1), (2, 2, 1))\n",
    "        self.c4 = nn.Conv3d(1, 1, (3, 3, 1), (2, 2, 1))\n",
    "        self.lstm = nn.LSTM(input_size=583, hidden_size=64, batch_first=True)\n",
    "        self.lin = nn.Linear(64, 267)\n",
    "    \n",
    "    def forward(self, lcd, spec, led):\n",
    "        cled = self.c1_1(led)\n",
    "        cspec = self.c1_2(spec)\n",
    "        clcd1 = self.c2(lcd)\n",
    "        clcd2 = self.c3(clcd1)\n",
    "        clcd3 = self.c4(clcd2)\n",
    "        clcd4 = self.c4(clcd3)\n",
    "        clcd5 = self.c4(clcd4)\n",
    "        ylcd = torch.reshape(clcd5, clcd5.shape[:1] + (-1,))\n",
    "        yspec = torch.reshape(cspec, cspec.shape[:1] + (-1,))\n",
    "        yled = torch.reshape(cled, cled.shape[:1] + (-1,))\n",
    "        y0 = torch.cat([ylcd, yspec, yled], 1)\n",
    "        y_rnn, h = self.lstm(y0, None)\n",
    "        y = self.lin(y_rnn)\n",
    "        return y\n",
    "\n",
    "net = Net()\n",
    "net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:\t0\tloss_train=40554.9453125\n",
      "epoch:\t1\tloss_train=43018.25390625\n",
      "epoch:\t2\tloss_train=37116.02734375\n",
      "epoch:\t3\tloss_train=24659.580078125\n",
      "epoch:\t4\tloss_train=37295.83984375\n",
      "epoch:\t5\tloss_train=34040.8046875\n",
      "epoch:\t6\tloss_train=18695.5859375\n",
      "epoch:\t7\tloss_train=20375.572265625\n",
      "epoch:\t8\tloss_train=29154.958984375\n",
      "epoch:\t9\tloss_train=21997.498046875\n",
      "epoch:\t10\tloss_train=25202.30078125\n",
      "epoch:\t11\tloss_train=27062.83984375\n",
      "epoch:\t12\tloss_train=26526.748046875\n",
      "epoch:\t13\tloss_train=25273.1640625\n",
      "epoch:\t14\tloss_train=21509.66015625\n",
      "epoch:\t15\tloss_train=14719.8603515625\n",
      "epoch:\t16\tloss_train=16456.759765625\n",
      "epoch:\t17\tloss_train=14949.7802734375\n",
      "epoch:\t18\tloss_train=10336.99609375\n",
      "epoch:\t19\tloss_train=9161.630859375\n",
      "epoch:\t20\tloss_train=14279.150390625\n",
      "epoch:\t21\tloss_train=12044.2509765625\n",
      "epoch:\t22\tloss_train=9546.791015625\n",
      "epoch:\t23\tloss_train=9208.7197265625\n",
      "epoch:\t24\tloss_train=17838.671875\n",
      "epoch:\t25\tloss_train=10039.7060546875\n",
      "epoch:\t26\tloss_train=11950.72265625\n",
      "epoch:\t27\tloss_train=9041.5615234375\n",
      "epoch:\t28\tloss_train=10163.4931640625\n",
      "epoch:\t29\tloss_train=10511.662109375\n",
      "epoch:\t30\tloss_train=8925.541015625\n",
      "epoch:\t31\tloss_train=9106.0048828125\n",
      "epoch:\t32\tloss_train=7889.33447265625\n",
      "epoch:\t33\tloss_train=6393.462890625\n",
      "epoch:\t34\tloss_train=16112.9541015625\n",
      "epoch:\t35\tloss_train=7962.810546875\n",
      "epoch:\t36\tloss_train=5114.24853515625\n",
      "epoch:\t37\tloss_train=8187.99609375\n",
      "epoch:\t38\tloss_train=7599.83642578125\n",
      "epoch:\t39\tloss_train=7744.40771484375\n",
      "epoch:\t40\tloss_train=9255.0498046875\n",
      "epoch:\t41\tloss_train=13206.8671875\n",
      "epoch:\t42\tloss_train=5429.2763671875\n",
      "epoch:\t43\tloss_train=5106.921875\n",
      "epoch:\t44\tloss_train=5229.78955078125\n",
      "epoch:\t45\tloss_train=4904.25\n",
      "epoch:\t46\tloss_train=8060.6962890625\n",
      "epoch:\t47\tloss_train=9108.0068359375\n",
      "epoch:\t48\tloss_train=7642.29248046875\n",
      "epoch:\t49\tloss_train=5598.86962890625\n",
      "epoch:\t50\tloss_train=6646.33056640625\n",
      "epoch:\t51\tloss_train=6611.92529296875\n",
      "epoch:\t52\tloss_train=9450.791015625\n",
      "epoch:\t53\tloss_train=8091.796875\n",
      "epoch:\t54\tloss_train=3657.507080078125\n",
      "epoch:\t55\tloss_train=3552.54345703125\n",
      "epoch:\t56\tloss_train=6077.6728515625\n",
      "epoch:\t57\tloss_train=6072.63623046875\n",
      "epoch:\t58\tloss_train=4936.37353515625\n",
      "epoch:\t59\tloss_train=6070.2119140625\n",
      "epoch:\t60\tloss_train=8680.046875\n",
      "epoch:\t61\tloss_train=5217.08642578125\n",
      "epoch:\t62\tloss_train=4017.7392578125\n",
      "epoch:\t63\tloss_train=3670.5\n",
      "epoch:\t64\tloss_train=3814.854736328125\n",
      "epoch:\t65\tloss_train=6322.162109375\n",
      "epoch:\t66\tloss_train=8131.4267578125\n",
      "epoch:\t67\tloss_train=4940.37353515625\n",
      "epoch:\t68\tloss_train=2675.004638671875\n",
      "epoch:\t69\tloss_train=3818.834228515625\n",
      "epoch:\t70\tloss_train=1397.4295654296875\n",
      "epoch:\t71\tloss_train=7834.6923828125\n",
      "epoch:\t72\tloss_train=1254.716064453125\n",
      "epoch:\t73\tloss_train=7378.228515625\n",
      "epoch:\t74\tloss_train=2573.5458984375\n",
      "epoch:\t75\tloss_train=6861.37060546875\n",
      "epoch:\t76\tloss_train=4119.31787109375\n",
      "epoch:\t77\tloss_train=3609.00927734375\n",
      "epoch:\t78\tloss_train=8985.7802734375\n",
      "epoch:\t79\tloss_train=7863.4541015625\n",
      "epoch:\t80\tloss_train=4577.2939453125\n",
      "epoch:\t81\tloss_train=5690.93798828125\n",
      "epoch:\t82\tloss_train=3618.815185546875\n",
      "epoch:\t83\tloss_train=6589.06787109375\n",
      "epoch:\t84\tloss_train=5297.7626953125\n",
      "epoch:\t85\tloss_train=7617.70751953125\n",
      "epoch:\t86\tloss_train=4193.732421875\n",
      "epoch:\t87\tloss_train=10925.904296875\n",
      "epoch:\t88\tloss_train=5111.177734375\n",
      "epoch:\t89\tloss_train=7500.10107421875\n",
      "epoch:\t90\tloss_train=1949.648681640625\n",
      "epoch:\t91\tloss_train=1736.26806640625\n",
      "epoch:\t92\tloss_train=7605.1904296875\n",
      "epoch:\t93\tloss_train=745.0687866210938\n",
      "epoch:\t94\tloss_train=4681.23193359375\n",
      "epoch:\t95\tloss_train=1535.0987548828125\n",
      "epoch:\t96\tloss_train=1491.4205322265625\n",
      "epoch:\t97\tloss_train=6420.732421875\n",
      "epoch:\t98\tloss_train=4496.11181640625\n",
      "epoch:\t99\tloss_train=9348.0673828125\n",
      "epoch:\t100\tloss_train=8459.2509765625\n",
      "epoch:\t101\tloss_train=1642.849609375\n",
      "epoch:\t102\tloss_train=9473.95703125\n",
      "epoch:\t103\tloss_train=3220.0556640625\n",
      "epoch:\t104\tloss_train=4203.52587890625\n",
      "epoch:\t105\tloss_train=6938.63232421875\n",
      "epoch:\t106\tloss_train=2468.831787109375\n",
      "epoch:\t107\tloss_train=6943.2431640625\n",
      "epoch:\t108\tloss_train=5186.23828125\n",
      "epoch:\t109\tloss_train=7989.11767578125\n",
      "epoch:\t110\tloss_train=526.7742919921875\n",
      "epoch:\t111\tloss_train=6344.828125\n",
      "epoch:\t112\tloss_train=7640.455078125\n",
      "epoch:\t113\tloss_train=3299.42138671875\n",
      "epoch:\t114\tloss_train=5855.34765625\n",
      "epoch:\t115\tloss_train=6661.91162109375\n",
      "epoch:\t116\tloss_train=1189.811767578125\n",
      "epoch:\t117\tloss_train=1682.2037353515625\n",
      "epoch:\t118\tloss_train=7763.29541015625\n",
      "epoch:\t119\tloss_train=5819.96875\n",
      "epoch:\t120\tloss_train=12349.5908203125\n",
      "epoch:\t121\tloss_train=5266.591796875\n",
      "epoch:\t122\tloss_train=2603.885986328125\n",
      "epoch:\t123\tloss_train=6781.462890625\n",
      "epoch:\t124\tloss_train=4363.9609375\n",
      "epoch:\t125\tloss_train=3583.710693359375\n",
      "epoch:\t126\tloss_train=2257.1689453125\n",
      "epoch:\t127\tloss_train=8948.32421875\n",
      "epoch:\t128\tloss_train=2587.490478515625\n",
      "epoch:\t129\tloss_train=1333.595703125\n",
      "epoch:\t130\tloss_train=1225.9488525390625\n",
      "epoch:\t131\tloss_train=5014.763671875\n",
      "epoch:\t132\tloss_train=7415.97021484375\n",
      "epoch:\t133\tloss_train=5645.486328125\n",
      "epoch:\t134\tloss_train=4639.0205078125\n",
      "epoch:\t135\tloss_train=13074.7119140625\n",
      "epoch:\t136\tloss_train=6122.18212890625\n",
      "epoch:\t137\tloss_train=13481.810546875\n",
      "epoch:\t138\tloss_train=2524.2431640625\n",
      "epoch:\t139\tloss_train=5424.01513671875\n",
      "epoch:\t140\tloss_train=573.8215942382812\n",
      "epoch:\t141\tloss_train=3866.9296875\n",
      "epoch:\t142\tloss_train=8101.60205078125\n",
      "epoch:\t143\tloss_train=3944.707763671875\n",
      "epoch:\t144\tloss_train=919.7523193359375\n",
      "epoch:\t145\tloss_train=1084.133544921875\n",
      "epoch:\t146\tloss_train=5577.919921875\n",
      "epoch:\t147\tloss_train=1259.4508056640625\n",
      "epoch:\t148\tloss_train=3002.440673828125\n",
      "epoch:\t149\tloss_train=5594.896484375\n",
      "epoch:\t150\tloss_train=4396.0\n",
      "epoch:\t151\tloss_train=4039.537841796875\n",
      "epoch:\t152\tloss_train=4213.49365234375\n",
      "epoch:\t153\tloss_train=946.3258666992188\n",
      "epoch:\t154\tloss_train=3960.718994140625\n",
      "epoch:\t155\tloss_train=4208.15478515625\n",
      "epoch:\t156\tloss_train=5462.853515625\n",
      "epoch:\t157\tloss_train=2779.357666015625\n",
      "epoch:\t158\tloss_train=5084.7568359375\n",
      "epoch:\t159\tloss_train=5433.16259765625\n",
      "epoch:\t160\tloss_train=477.79290771484375\n",
      "epoch:\t161\tloss_train=5785.6787109375\n",
      "epoch:\t162\tloss_train=5335.12841796875\n",
      "epoch:\t163\tloss_train=3363.56884765625\n",
      "epoch:\t164\tloss_train=6527.9619140625\n",
      "epoch:\t165\tloss_train=972.1983032226562\n",
      "epoch:\t166\tloss_train=7057.01904296875\n",
      "epoch:\t167\tloss_train=3972.14892578125\n",
      "epoch:\t168\tloss_train=6123.87109375\n",
      "epoch:\t169\tloss_train=2085.21533203125\n",
      "epoch:\t170\tloss_train=3827.591552734375\n",
      "epoch:\t171\tloss_train=15594.19921875\n",
      "epoch:\t172\tloss_train=4946.87060546875\n",
      "epoch:\t173\tloss_train=10004.3291015625\n",
      "epoch:\t174\tloss_train=5505.14794921875\n",
      "epoch:\t175\tloss_train=8409.669921875\n",
      "epoch:\t176\tloss_train=5360.1044921875\n",
      "epoch:\t177\tloss_train=6810.48974609375\n",
      "epoch:\t178\tloss_train=7807.5517578125\n",
      "epoch:\t179\tloss_train=2443.1669921875\n",
      "epoch:\t180\tloss_train=6118.79150390625\n",
      "epoch:\t181\tloss_train=1155.7333984375\n",
      "epoch:\t182\tloss_train=8481.447265625\n",
      "epoch:\t183\tloss_train=812.3572998046875\n",
      "epoch:\t184\tloss_train=2038.4769287109375\n",
      "epoch:\t185\tloss_train=5944.29736328125\n",
      "epoch:\t186\tloss_train=2338.8486328125\n",
      "epoch:\t187\tloss_train=6547.9404296875\n",
      "epoch:\t188\tloss_train=8375.505859375\n",
      "epoch:\t189\tloss_train=4687.37646484375\n",
      "epoch:\t190\tloss_train=11382.814453125\n",
      "epoch:\t191\tloss_train=2679.9150390625\n",
      "epoch:\t192\tloss_train=4759.80078125\n",
      "epoch:\t193\tloss_train=3463.79931640625\n",
      "epoch:\t194\tloss_train=5430.47412109375\n",
      "epoch:\t195\tloss_train=5392.39501953125\n",
      "epoch:\t196\tloss_train=5858.173828125\n",
      "epoch:\t197\tloss_train=2512.634033203125\n",
      "epoch:\t198\tloss_train=1734.86083984375\n",
      "epoch:\t199\tloss_train=7338.28564453125\n",
      "epoch:\t200\tloss_train=10667.736328125\n",
      "epoch:\t201\tloss_train=4220.31005859375\n",
      "epoch:\t202\tloss_train=6471.60791015625\n",
      "epoch:\t203\tloss_train=10649.6376953125\n",
      "epoch:\t204\tloss_train=2712.82373046875\n",
      "epoch:\t205\tloss_train=6481.18017578125\n",
      "epoch:\t206\tloss_train=772.6929321289062\n",
      "epoch:\t207\tloss_train=4290.59326171875\n",
      "epoch:\t208\tloss_train=1044.7381591796875\n",
      "epoch:\t209\tloss_train=5723.177734375\n",
      "epoch:\t210\tloss_train=1403.5528564453125\n",
      "epoch:\t211\tloss_train=2050.658935546875\n",
      "epoch:\t212\tloss_train=1482.3497314453125\n",
      "epoch:\t213\tloss_train=8146.359375\n",
      "epoch:\t214\tloss_train=6375.03466796875\n",
      "epoch:\t215\tloss_train=6307.32177734375\n",
      "epoch:\t216\tloss_train=4908.763671875\n",
      "epoch:\t217\tloss_train=1712.9803466796875\n",
      "epoch:\t218\tloss_train=1225.940673828125\n",
      "epoch:\t219\tloss_train=7383.99609375\n",
      "epoch:\t220\tloss_train=3824.33251953125\n",
      "epoch:\t221\tloss_train=428.5847473144531\n",
      "epoch:\t222\tloss_train=1112.9932861328125\n",
      "epoch:\t223\tloss_train=5310.08056640625\n",
      "epoch:\t224\tloss_train=8729.689453125\n",
      "epoch:\t225\tloss_train=4281.80029296875\n",
      "epoch:\t226\tloss_train=2033.17138671875\n",
      "epoch:\t227\tloss_train=5287.9697265625\n",
      "epoch:\t228\tloss_train=3123.193603515625\n",
      "epoch:\t229\tloss_train=6372.546875\n",
      "epoch:\t230\tloss_train=1839.3204345703125\n",
      "epoch:\t231\tloss_train=5817.30224609375\n",
      "epoch:\t232\tloss_train=6872.78955078125\n",
      "epoch:\t233\tloss_train=2450.697021484375\n",
      "epoch:\t234\tloss_train=6306.9150390625\n",
      "epoch:\t235\tloss_train=6984.734375\n",
      "epoch:\t236\tloss_train=4934.91943359375\n",
      "epoch:\t237\tloss_train=5514.22998046875\n",
      "epoch:\t238\tloss_train=6874.72265625\n",
      "epoch:\t239\tloss_train=2779.48583984375\n",
      "epoch:\t240\tloss_train=3851.6689453125\n",
      "epoch:\t241\tloss_train=1339.2081298828125\n",
      "epoch:\t242\tloss_train=12961.994140625\n",
      "epoch:\t243\tloss_train=4445.4560546875\n",
      "epoch:\t244\tloss_train=552.1573486328125\n",
      "epoch:\t245\tloss_train=3199.765869140625\n",
      "epoch:\t246\tloss_train=4100.560546875\n",
      "epoch:\t247\tloss_train=8199.0703125\n",
      "epoch:\t248\tloss_train=2944.304931640625\n",
      "epoch:\t249\tloss_train=2905.8046875\n",
      "epoch:\t250\tloss_train=1328.701904296875\n",
      "epoch:\t251\tloss_train=962.639892578125\n",
      "epoch:\t252\tloss_train=2011.7762451171875\n",
      "epoch:\t253\tloss_train=1822.215576171875\n",
      "epoch:\t254\tloss_train=863.167724609375\n",
      "epoch:\t255\tloss_train=1660.779052734375\n",
      "epoch:\t256\tloss_train=2179.618408203125\n",
      "epoch:\t257\tloss_train=11511.12890625\n",
      "epoch:\t258\tloss_train=645.0303955078125\n",
      "epoch:\t259\tloss_train=757.2992553710938\n",
      "epoch:\t260\tloss_train=7441.02734375\n",
      "epoch:\t261\tloss_train=6420.2734375\n",
      "epoch:\t262\tloss_train=682.08837890625\n",
      "epoch:\t263\tloss_train=6181.89404296875\n",
      "epoch:\t264\tloss_train=7727.36572265625\n",
      "epoch:\t265\tloss_train=6709.4873046875\n",
      "epoch:\t266\tloss_train=3879.94091796875\n",
      "epoch:\t267\tloss_train=2453.53125\n",
      "epoch:\t268\tloss_train=1961.8536376953125\n",
      "epoch:\t269\tloss_train=7152.96240234375\n",
      "epoch:\t270\tloss_train=8262.236328125\n",
      "epoch:\t271\tloss_train=7986.6083984375\n",
      "epoch:\t272\tloss_train=7418.32177734375\n",
      "epoch:\t273\tloss_train=4843.3076171875\n",
      "epoch:\t274\tloss_train=6490.89306640625\n",
      "epoch:\t275\tloss_train=2989.886474609375\n",
      "epoch:\t276\tloss_train=1212.493896484375\n",
      "epoch:\t277\tloss_train=7958.62744140625\n",
      "epoch:\t278\tloss_train=4340.74658203125\n",
      "epoch:\t279\tloss_train=5455.02734375\n",
      "epoch:\t280\tloss_train=10874.9658203125\n",
      "epoch:\t281\tloss_train=2572.65966796875\n",
      "epoch:\t282\tloss_train=538.65673828125\n",
      "epoch:\t283\tloss_train=847.2349853515625\n",
      "epoch:\t284\tloss_train=3212.280029296875\n",
      "epoch:\t285\tloss_train=6809.2626953125\n",
      "epoch:\t286\tloss_train=8320.6796875\n",
      "epoch:\t287\tloss_train=2126.882568359375\n",
      "epoch:\t288\tloss_train=13793.34765625\n",
      "epoch:\t289\tloss_train=2015.287841796875\n",
      "epoch:\t290\tloss_train=2051.790283203125\n",
      "epoch:\t291\tloss_train=4575.814453125\n",
      "epoch:\t292\tloss_train=606.5784301757812\n",
      "epoch:\t293\tloss_train=3528.914306640625\n",
      "epoch:\t294\tloss_train=6400.29931640625\n",
      "epoch:\t295\tloss_train=1470.377685546875\n",
      "epoch:\t296\tloss_train=2896.5654296875\n",
      "epoch:\t297\tloss_train=1308.2786865234375\n",
      "epoch:\t298\tloss_train=4806.89892578125\n",
      "epoch:\t299\tloss_train=5120.056640625\n",
      "epoch:\t300\tloss_train=4897.740234375\n",
      "epoch:\t301\tloss_train=1428.6270751953125\n",
      "epoch:\t302\tloss_train=615.78271484375\n",
      "epoch:\t303\tloss_train=1474.2249755859375\n",
      "epoch:\t304\tloss_train=1144.42724609375\n",
      "epoch:\t305\tloss_train=1023.8604736328125\n",
      "epoch:\t306\tloss_train=3705.0576171875\n",
      "epoch:\t307\tloss_train=10156.1865234375\n",
      "epoch:\t308\tloss_train=4762.45556640625\n",
      "epoch:\t309\tloss_train=4439.1826171875\n",
      "epoch:\t310\tloss_train=2676.96923828125\n",
      "epoch:\t311\tloss_train=2995.001708984375\n",
      "epoch:\t312\tloss_train=1882.8038330078125\n",
      "epoch:\t313\tloss_train=2424.7939453125\n",
      "epoch:\t314\tloss_train=1031.7327880859375\n",
      "epoch:\t315\tloss_train=1330.5806884765625\n",
      "epoch:\t316\tloss_train=1305.9527587890625\n",
      "epoch:\t317\tloss_train=3569.1572265625\n",
      "epoch:\t318\tloss_train=703.5557250976562\n",
      "epoch:\t319\tloss_train=3593.8388671875\n",
      "epoch:\t320\tloss_train=539.7057495117188\n",
      "epoch:\t321\tloss_train=1065.033447265625\n",
      "epoch:\t322\tloss_train=3812.807861328125\n",
      "epoch:\t323\tloss_train=3685.951904296875\n",
      "epoch:\t324\tloss_train=2609.71533203125\n",
      "epoch:\t325\tloss_train=2498.153564453125\n",
      "epoch:\t326\tloss_train=1311.9990234375\n",
      "epoch:\t327\tloss_train=875.3168334960938\n",
      "epoch:\t328\tloss_train=1699.858642578125\n",
      "epoch:\t329\tloss_train=6960.26171875\n",
      "epoch:\t330\tloss_train=3841.367431640625\n",
      "epoch:\t331\tloss_train=7437.2138671875\n",
      "epoch:\t332\tloss_train=9812.9853515625\n",
      "epoch:\t333\tloss_train=1946.6011962890625\n",
      "epoch:\t334\tloss_train=4489.9609375\n",
      "epoch:\t335\tloss_train=2044.6851806640625\n",
      "epoch:\t336\tloss_train=5198.26708984375\n",
      "epoch:\t337\tloss_train=5597.9111328125\n",
      "epoch:\t338\tloss_train=3009.053466796875\n",
      "epoch:\t339\tloss_train=10639.5234375\n",
      "epoch:\t340\tloss_train=10249.00390625\n",
      "epoch:\t341\tloss_train=7476.39794921875\n",
      "epoch:\t342\tloss_train=6656.60107421875\n",
      "epoch:\t343\tloss_train=6644.337890625\n",
      "epoch:\t344\tloss_train=5916.2265625\n",
      "epoch:\t345\tloss_train=942.7015991210938\n",
      "epoch:\t346\tloss_train=5523.2041015625\n",
      "epoch:\t347\tloss_train=875.3023681640625\n",
      "epoch:\t348\tloss_train=3587.785888671875\n",
      "epoch:\t349\tloss_train=8709.8046875\n",
      "epoch:\t350\tloss_train=6749.01611328125\n",
      "epoch:\t351\tloss_train=2010.061767578125\n",
      "epoch:\t352\tloss_train=837.6998901367188\n",
      "epoch:\t353\tloss_train=6509.80908203125\n",
      "epoch:\t354\tloss_train=1873.7498779296875\n",
      "epoch:\t355\tloss_train=1730.3509521484375\n",
      "epoch:\t356\tloss_train=3963.942626953125\n",
      "epoch:\t357\tloss_train=3990.083984375\n",
      "epoch:\t358\tloss_train=8173.88232421875\n",
      "epoch:\t359\tloss_train=1880.7535400390625\n",
      "epoch:\t360\tloss_train=6332.7001953125\n",
      "epoch:\t361\tloss_train=839.6886596679688\n",
      "epoch:\t362\tloss_train=6520.60009765625\n",
      "epoch:\t363\tloss_train=2084.390380859375\n",
      "epoch:\t364\tloss_train=1799.9459228515625\n",
      "epoch:\t365\tloss_train=1201.13134765625\n",
      "epoch:\t366\tloss_train=5478.56689453125\n",
      "epoch:\t367\tloss_train=3757.966064453125\n",
      "epoch:\t368\tloss_train=673.3136596679688\n",
      "epoch:\t369\tloss_train=479.1272277832031\n",
      "epoch:\t370\tloss_train=14644.810546875\n",
      "epoch:\t371\tloss_train=7079.84716796875\n",
      "epoch:\t372\tloss_train=2029.0882568359375\n",
      "epoch:\t373\tloss_train=4289.97900390625\n",
      "epoch:\t374\tloss_train=6438.52783203125\n",
      "epoch:\t375\tloss_train=4604.4921875\n",
      "epoch:\t376\tloss_train=6714.20166015625\n",
      "epoch:\t377\tloss_train=8524.564453125\n",
      "epoch:\t378\tloss_train=2168.484619140625\n",
      "epoch:\t379\tloss_train=1147.116455078125\n",
      "epoch:\t380\tloss_train=5496.85498046875\n",
      "epoch:\t381\tloss_train=1431.7935791015625\n",
      "epoch:\t382\tloss_train=4574.10693359375\n",
      "epoch:\t383\tloss_train=5647.43115234375\n",
      "epoch:\t384\tloss_train=4836.671875\n",
      "epoch:\t385\tloss_train=1105.6580810546875\n",
      "epoch:\t386\tloss_train=5097.09912109375\n",
      "epoch:\t387\tloss_train=1560.6451416015625\n",
      "epoch:\t388\tloss_train=2769.097900390625\n",
      "epoch:\t389\tloss_train=7204.98486328125\n",
      "epoch:\t390\tloss_train=1914.2772216796875\n",
      "epoch:\t391\tloss_train=3812.90625\n",
      "epoch:\t392\tloss_train=1767.383056640625\n",
      "epoch:\t393\tloss_train=4413.45556640625\n",
      "epoch:\t394\tloss_train=2932.8125\n",
      "epoch:\t395\tloss_train=3404.796630859375\n",
      "epoch:\t396\tloss_train=2078.683837890625\n",
      "epoch:\t397\tloss_train=1356.262939453125\n",
      "epoch:\t398\tloss_train=5559.55908203125\n",
      "epoch:\t399\tloss_train=111.86736297607422\n",
      "epoch:\t400\tloss_train=2021.1607666015625\n",
      "epoch:\t401\tloss_train=2867.328125\n",
      "epoch:\t402\tloss_train=7565.63916015625\n",
      "epoch:\t403\tloss_train=5484.01025390625\n",
      "epoch:\t404\tloss_train=1999.138671875\n",
      "epoch:\t405\tloss_train=2775.934326171875\n",
      "epoch:\t406\tloss_train=10275.5986328125\n",
      "epoch:\t407\tloss_train=9806.404296875\n",
      "epoch:\t408\tloss_train=2306.472412109375\n",
      "epoch:\t409\tloss_train=7848.70458984375\n",
      "epoch:\t410\tloss_train=1282.5625\n",
      "epoch:\t411\tloss_train=3947.931640625\n",
      "epoch:\t412\tloss_train=4890.900390625\n",
      "epoch:\t413\tloss_train=6117.16943359375\n",
      "epoch:\t414\tloss_train=5200.2568359375\n",
      "epoch:\t415\tloss_train=3856.984130859375\n",
      "epoch:\t416\tloss_train=1565.78662109375\n",
      "epoch:\t417\tloss_train=4883.68896484375\n",
      "epoch:\t418\tloss_train=3273.453369140625\n",
      "epoch:\t419\tloss_train=2902.177490234375\n",
      "epoch:\t420\tloss_train=3065.7890625\n",
      "epoch:\t421\tloss_train=1021.4739379882812\n",
      "epoch:\t422\tloss_train=2027.13623046875\n",
      "epoch:\t423\tloss_train=6599.830078125\n",
      "epoch:\t424\tloss_train=1214.44873046875\n",
      "epoch:\t425\tloss_train=2981.071044921875\n",
      "epoch:\t426\tloss_train=5502.53955078125\n",
      "epoch:\t427\tloss_train=927.3739013671875\n",
      "epoch:\t428\tloss_train=3331.7314453125\n",
      "epoch:\t429\tloss_train=7666.64111328125\n",
      "epoch:\t430\tloss_train=5416.46728515625\n",
      "epoch:\t431\tloss_train=12294.4951171875\n",
      "epoch:\t432\tloss_train=425.37066650390625\n",
      "epoch:\t433\tloss_train=4477.865234375\n",
      "epoch:\t434\tloss_train=5613.89599609375\n",
      "epoch:\t435\tloss_train=3965.516357421875\n",
      "epoch:\t436\tloss_train=4328.47021484375\n",
      "epoch:\t437\tloss_train=8401.544921875\n",
      "epoch:\t438\tloss_train=1540.202880859375\n",
      "epoch:\t439\tloss_train=8068.361328125\n",
      "epoch:\t440\tloss_train=7915.873046875\n",
      "epoch:\t441\tloss_train=11813.033203125\n",
      "epoch:\t442\tloss_train=4001.375\n",
      "epoch:\t443\tloss_train=928.0413818359375\n",
      "epoch:\t444\tloss_train=3997.234130859375\n",
      "epoch:\t445\tloss_train=952.5536499023438\n",
      "epoch:\t446\tloss_train=4900.423828125\n",
      "epoch:\t447\tloss_train=788.02099609375\n",
      "epoch:\t448\tloss_train=1078.2938232421875\n",
      "epoch:\t449\tloss_train=433.22625732421875\n",
      "epoch:\t450\tloss_train=2229.473388671875\n",
      "epoch:\t451\tloss_train=2423.418212890625\n",
      "epoch:\t452\tloss_train=5093.5634765625\n",
      "epoch:\t453\tloss_train=3736.546630859375\n",
      "epoch:\t454\tloss_train=4680.12841796875\n",
      "epoch:\t455\tloss_train=2200.759033203125\n",
      "epoch:\t456\tloss_train=4714.4052734375\n",
      "epoch:\t457\tloss_train=1024.8099365234375\n",
      "epoch:\t458\tloss_train=1972.0433349609375\n",
      "epoch:\t459\tloss_train=989.1701049804688\n",
      "epoch:\t460\tloss_train=2629.56884765625\n",
      "epoch:\t461\tloss_train=4874.453125\n",
      "epoch:\t462\tloss_train=1305.2510986328125\n",
      "epoch:\t463\tloss_train=10260.48828125\n",
      "epoch:\t464\tloss_train=4069.642333984375\n",
      "epoch:\t465\tloss_train=5159.80322265625\n",
      "epoch:\t466\tloss_train=1814.7984619140625\n",
      "epoch:\t467\tloss_train=798.224853515625\n",
      "epoch:\t468\tloss_train=9074.439453125\n",
      "epoch:\t469\tloss_train=6754.3837890625\n",
      "epoch:\t470\tloss_train=1069.564697265625\n",
      "epoch:\t471\tloss_train=1474.6226806640625\n",
      "epoch:\t472\tloss_train=5087.4443359375\n",
      "epoch:\t473\tloss_train=1386.443359375\n",
      "epoch:\t474\tloss_train=5448.87939453125\n",
      "epoch:\t475\tloss_train=3646.82958984375\n",
      "epoch:\t476\tloss_train=839.564697265625\n",
      "epoch:\t477\tloss_train=3541.911376953125\n",
      "epoch:\t478\tloss_train=3252.01025390625\n",
      "epoch:\t479\tloss_train=3761.2568359375\n",
      "epoch:\t480\tloss_train=379.0616760253906\n",
      "epoch:\t481\tloss_train=1611.2166748046875\n",
      "epoch:\t482\tloss_train=1326.063720703125\n",
      "epoch:\t483\tloss_train=8557.912109375\n",
      "epoch:\t484\tloss_train=6753.1171875\n",
      "epoch:\t485\tloss_train=3071.8857421875\n",
      "epoch:\t486\tloss_train=2093.136962890625\n",
      "epoch:\t487\tloss_train=1289.7998046875\n",
      "epoch:\t488\tloss_train=10721.7255859375\n",
      "epoch:\t489\tloss_train=1174.8226318359375\n",
      "epoch:\t490\tloss_train=4278.06298828125\n",
      "epoch:\t491\tloss_train=772.7546997070312\n",
      "epoch:\t492\tloss_train=2212.968994140625\n",
      "epoch:\t493\tloss_train=2596.462646484375\n",
      "epoch:\t494\tloss_train=1351.5\n",
      "epoch:\t495\tloss_train=10659.9609375\n",
      "epoch:\t496\tloss_train=5652.78271484375\n",
      "epoch:\t497\tloss_train=6840.75048828125\n",
      "epoch:\t498\tloss_train=3683.576904296875\n",
      "epoch:\t499\tloss_train=6084.33447265625\n",
      "epoch:\t500\tloss_train=2509.2548828125\n",
      "epoch:\t501\tloss_train=446.8731994628906\n",
      "epoch:\t502\tloss_train=5561.4677734375\n",
      "epoch:\t503\tloss_train=416.3211669921875\n",
      "epoch:\t504\tloss_train=6815.28515625\n",
      "epoch:\t505\tloss_train=2907.048583984375\n",
      "epoch:\t506\tloss_train=5323.8486328125\n",
      "epoch:\t507\tloss_train=8407.7880859375\n",
      "epoch:\t508\tloss_train=10468.05859375\n",
      "epoch:\t509\tloss_train=5202.19580078125\n",
      "epoch:\t510\tloss_train=948.3902587890625\n",
      "epoch:\t511\tloss_train=1328.46044921875\n",
      "epoch:\t512\tloss_train=1386.4842529296875\n",
      "epoch:\t513\tloss_train=4183.99755859375\n",
      "epoch:\t514\tloss_train=4019.046875\n",
      "epoch:\t515\tloss_train=6612.78076171875\n",
      "epoch:\t516\tloss_train=6833.97900390625\n",
      "epoch:\t517\tloss_train=6565.5458984375\n",
      "epoch:\t518\tloss_train=1353.0721435546875\n",
      "epoch:\t519\tloss_train=1266.46240234375\n",
      "epoch:\t520\tloss_train=5952.17529296875\n",
      "epoch:\t521\tloss_train=7351.31640625\n",
      "epoch:\t522\tloss_train=5157.27783203125\n",
      "epoch:\t523\tloss_train=3436.24658203125\n",
      "epoch:\t524\tloss_train=6080.650390625\n",
      "epoch:\t525\tloss_train=7390.453125\n",
      "epoch:\t526\tloss_train=4156.48583984375\n",
      "epoch:\t527\tloss_train=1112.5079345703125\n",
      "epoch:\t528\tloss_train=574.8049926757812\n",
      "epoch:\t529\tloss_train=1703.2003173828125\n",
      "epoch:\t530\tloss_train=8956.744140625\n",
      "epoch:\t531\tloss_train=4609.85107421875\n",
      "epoch:\t532\tloss_train=2107.67822265625\n",
      "epoch:\t533\tloss_train=3798.83984375\n",
      "epoch:\t534\tloss_train=5588.52685546875\n",
      "epoch:\t535\tloss_train=1641.52197265625\n",
      "epoch:\t536\tloss_train=2042.5394287109375\n",
      "epoch:\t537\tloss_train=1265.2547607421875\n",
      "epoch:\t538\tloss_train=1668.833251953125\n",
      "epoch:\t539\tloss_train=3691.11279296875\n",
      "epoch:\t540\tloss_train=5475.6640625\n",
      "epoch:\t541\tloss_train=1372.8050537109375\n",
      "epoch:\t542\tloss_train=2859.671142578125\n",
      "epoch:\t543\tloss_train=1109.71826171875\n",
      "epoch:\t544\tloss_train=1782.8948974609375\n",
      "epoch:\t545\tloss_train=1145.1943359375\n",
      "epoch:\t546\tloss_train=2839.320068359375\n",
      "epoch:\t547\tloss_train=5317.3408203125\n",
      "epoch:\t548\tloss_train=1371.0726318359375\n",
      "epoch:\t549\tloss_train=601.1235961914062\n",
      "epoch:\t550\tloss_train=598.3707275390625\n",
      "epoch:\t551\tloss_train=8931.685546875\n",
      "epoch:\t552\tloss_train=442.5835266113281\n",
      "epoch:\t553\tloss_train=3607.848876953125\n",
      "epoch:\t554\tloss_train=3727.748291015625\n",
      "epoch:\t555\tloss_train=10769.7119140625\n",
      "epoch:\t556\tloss_train=10478.23046875\n",
      "epoch:\t557\tloss_train=1995.09814453125\n",
      "epoch:\t558\tloss_train=4002.385498046875\n",
      "epoch:\t559\tloss_train=1106.5133056640625\n",
      "epoch:\t560\tloss_train=1628.7484130859375\n",
      "epoch:\t561\tloss_train=859.6986083984375\n",
      "epoch:\t562\tloss_train=808.8901977539062\n",
      "epoch:\t563\tloss_train=6311.0439453125\n",
      "epoch:\t564\tloss_train=6742.275390625\n",
      "epoch:\t565\tloss_train=1108.49658203125\n",
      "epoch:\t566\tloss_train=8971.9775390625\n",
      "epoch:\t567\tloss_train=866.5244750976562\n",
      "epoch:\t568\tloss_train=1863.7093505859375\n",
      "epoch:\t569\tloss_train=1663.615966796875\n",
      "epoch:\t570\tloss_train=5534.6884765625\n",
      "epoch:\t571\tloss_train=933.2501831054688\n",
      "epoch:\t572\tloss_train=5066.6416015625\n",
      "epoch:\t573\tloss_train=1348.316650390625\n",
      "epoch:\t574\tloss_train=7894.67236328125\n",
      "epoch:\t575\tloss_train=6738.55810546875\n",
      "epoch:\t576\tloss_train=6014.43994140625\n",
      "epoch:\t577\tloss_train=1531.4373779296875\n",
      "epoch:\t578\tloss_train=2409.033447265625\n",
      "epoch:\t579\tloss_train=4768.353515625\n",
      "epoch:\t580\tloss_train=3837.283203125\n",
      "epoch:\t581\tloss_train=1981.0909423828125\n",
      "epoch:\t582\tloss_train=1358.324951171875\n",
      "epoch:\t583\tloss_train=4175.21142578125\n",
      "epoch:\t584\tloss_train=1125.859375\n",
      "epoch:\t585\tloss_train=8803.7841796875\n",
      "epoch:\t586\tloss_train=10064.4892578125\n",
      "epoch:\t587\tloss_train=1879.1209716796875\n",
      "epoch:\t588\tloss_train=1458.0382080078125\n",
      "epoch:\t589\tloss_train=1271.6302490234375\n",
      "epoch:\t590\tloss_train=4583.693359375\n",
      "epoch:\t591\tloss_train=2995.339599609375\n",
      "epoch:\t592\tloss_train=2129.9189453125\n",
      "epoch:\t593\tloss_train=2342.791015625\n",
      "epoch:\t594\tloss_train=1615.2138671875\n",
      "epoch:\t595\tloss_train=2551.5615234375\n",
      "epoch:\t596\tloss_train=4162.03125\n",
      "epoch:\t597\tloss_train=2270.0224609375\n",
      "epoch:\t598\tloss_train=3137.63232421875\n",
      "epoch:\t599\tloss_train=662.9817504882812\n",
      "epoch:\t600\tloss_train=1717.656494140625\n",
      "epoch:\t601\tloss_train=4210.09228515625\n",
      "epoch:\t602\tloss_train=1873.7926025390625\n",
      "epoch:\t603\tloss_train=643.0479736328125\n",
      "epoch:\t604\tloss_train=4483.296875\n",
      "epoch:\t605\tloss_train=2453.801513671875\n",
      "epoch:\t606\tloss_train=5315.7001953125\n",
      "epoch:\t607\tloss_train=4762.6240234375\n",
      "epoch:\t608\tloss_train=1069.4619140625\n",
      "epoch:\t609\tloss_train=901.960205078125\n",
      "epoch:\t610\tloss_train=5941.37255859375\n",
      "epoch:\t611\tloss_train=661.2328491210938\n",
      "epoch:\t612\tloss_train=776.03125\n",
      "epoch:\t613\tloss_train=9528.419921875\n",
      "epoch:\t614\tloss_train=4641.875\n",
      "epoch:\t615\tloss_train=5363.21142578125\n",
      "epoch:\t616\tloss_train=684.45556640625\n",
      "epoch:\t617\tloss_train=1043.1151123046875\n",
      "epoch:\t618\tloss_train=2510.04833984375\n",
      "epoch:\t619\tloss_train=2006.706298828125\n",
      "epoch:\t620\tloss_train=2869.53564453125\n",
      "epoch:\t621\tloss_train=3531.18701171875\n",
      "epoch:\t622\tloss_train=14438.26953125\n",
      "epoch:\t623\tloss_train=5307.7978515625\n",
      "epoch:\t624\tloss_train=1201.1982421875\n",
      "epoch:\t625\tloss_train=7433.40185546875\n",
      "epoch:\t626\tloss_train=169.92115783691406\n",
      "epoch:\t627\tloss_train=10511.5263671875\n",
      "epoch:\t628\tloss_train=5850.29931640625\n",
      "epoch:\t629\tloss_train=4551.9140625\n",
      "epoch:\t630\tloss_train=1406.2880859375\n",
      "epoch:\t631\tloss_train=897.901611328125\n",
      "epoch:\t632\tloss_train=3232.463623046875\n",
      "epoch:\t633\tloss_train=5634.31689453125\n",
      "epoch:\t634\tloss_train=1367.9249267578125\n",
      "epoch:\t635\tloss_train=1345.51806640625\n",
      "epoch:\t636\tloss_train=1995.3719482421875\n",
      "epoch:\t637\tloss_train=5096.0888671875\n",
      "epoch:\t638\tloss_train=9840.5478515625\n",
      "epoch:\t639\tloss_train=2081.685546875\n",
      "epoch:\t640\tloss_train=1465.9364013671875\n",
      "epoch:\t641\tloss_train=1307.749267578125\n",
      "epoch:\t642\tloss_train=6269.45458984375\n",
      "epoch:\t643\tloss_train=1173.693603515625\n",
      "epoch:\t644\tloss_train=8074.49951171875\n",
      "epoch:\t645\tloss_train=1470.956787109375\n",
      "epoch:\t646\tloss_train=1561.777099609375\n",
      "epoch:\t647\tloss_train=1985.637939453125\n",
      "epoch:\t648\tloss_train=7301.5244140625\n",
      "epoch:\t649\tloss_train=942.26904296875\n",
      "epoch:\t650\tloss_train=9912.82421875\n",
      "epoch:\t651\tloss_train=4076.103759765625\n",
      "epoch:\t652\tloss_train=4086.210693359375\n",
      "epoch:\t653\tloss_train=1130.83984375\n",
      "epoch:\t654\tloss_train=4872.166015625\n",
      "epoch:\t655\tloss_train=1407.6170654296875\n",
      "epoch:\t656\tloss_train=624.3728637695312\n",
      "epoch:\t657\tloss_train=933.6915893554688\n",
      "epoch:\t658\tloss_train=4165.3857421875\n",
      "epoch:\t659\tloss_train=1097.918212890625\n",
      "epoch:\t660\tloss_train=4921.83203125\n",
      "epoch:\t661\tloss_train=1119.80810546875\n",
      "epoch:\t662\tloss_train=4012.466552734375\n",
      "epoch:\t663\tloss_train=1427.8182373046875\n",
      "epoch:\t664\tloss_train=4630.3779296875\n",
      "epoch:\t665\tloss_train=15977.091796875\n",
      "epoch:\t666\tloss_train=1320.9925537109375\n",
      "epoch:\t667\tloss_train=4556.7626953125\n",
      "epoch:\t668\tloss_train=3617.353271484375\n",
      "epoch:\t669\tloss_train=2419.0556640625\n",
      "epoch:\t670\tloss_train=10196.099609375\n",
      "epoch:\t671\tloss_train=4944.57177734375\n",
      "epoch:\t672\tloss_train=4271.9033203125\n",
      "epoch:\t673\tloss_train=2813.14599609375\n",
      "epoch:\t674\tloss_train=3540.39501953125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     10\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mlcd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlcd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlcd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlcd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mled\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_fnc = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "record_loss_train = []\n",
    "epochs = 1000\n",
    "for i in range(epochs):\n",
    "    net.train()\n",
    "    loss_train = 0\n",
    "    for j, (lcd, spec, led, t) in enumerate(loader):\n",
    "        lcd, spec, led, t = lcd.cuda(), spec.cuda(), led.cuda(), t.cuda()\n",
    "        y = net(lcd, spec, led)\n",
    "        loss = loss_fnc(y, t)\n",
    "        loss_train += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_train /= j + 1\n",
    "    record_loss_train.append(loss_train)\n",
    "    print(\"epoch:\\t{}\\tloss_train={}\".format(i, loss))\n",
    "\n",
    "    # if i % 10 == 0 or i == epochs - 1:\n",
    "    #     net.eval()\n",
    "    #     print(\"Epoch: \", i, \"Loss_Train: \", loss_train)\n",
    "    #     predicted = list(input_data[0].view(-1))\n",
    "    #     for i in range(n_sample):\n",
    "    #         x = torch.tensor(predicted[-n_time:])\n",
    "    #         x = x.view(1, n_time, 1)\n",
    "    #         y = net(x)\n",
    "    #         predicted.append(y[0].item())\n",
    "    #     plt.plot(range(len(sin_y)), sin_y, label=\"Correct\")\n",
    "    #     plt.plot(range(len(predicted)), predicted, label=\"Predicted\")\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
