{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([840, 10, 320, 436, 3])\n",
      "torch.Size([840, 10, 128])\n",
      "torch.Size([840, 9, 267])\n",
      "torch.Size([840, 267])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "\n",
    "n = 6\n",
    "data_id = 1\n",
    "n_time = 10\n",
    "frame_max = 150\n",
    "n_sample = frame_max - n_time\n",
    "\n",
    "led_data = \"../data/dataset/led/led_{}.csv\".format(data_id)\n",
    "lcd_data = \"../data/dataset/lcd/lcd_{}.mp4\".format(data_id)\n",
    "spec_data = \"../data/dataset/spc/spec_{}.csv\".format(data_id)\n",
    "led = np.loadtxt(led_data, dtype=np.int32)\n",
    "spec = np.loadtxt(spec_data, dtype=np.float32)\n",
    "lcd = cv2.VideoCapture(lcd_data)\n",
    "lcd_shape = (int(lcd.get(cv2.CAP_PROP_FRAME_WIDTH)), int(lcd.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "lcd_data = torch.zeros((n, frame_max, lcd_shape[1], lcd_shape[0], 3), dtype=torch.uint8)\n",
    "spec_data = torch.zeros((n, frame_max, spec.shape[0]))\n",
    "led_data = torch.zeros((n, frame_max, led.shape[0]))\n",
    "lcd_dataset = torch.zeros((n_sample * n, n_time, lcd_shape[1], lcd_shape[0], 3))\n",
    "spec_dataset = torch.zeros((n_sample * n, n_time, spec.shape[0]))\n",
    "led_dataset = torch.zeros((n_sample * n, n_time - 1, led.shape[0]))\n",
    "led_correct = torch.zeros(n_sample * n, led.shape[0])\n",
    "\n",
    "print(lcd_dataset.shape)\n",
    "print(spec_dataset.shape)\n",
    "print(led_dataset.shape)\n",
    "print(led_correct.shape)\n",
    "\n",
    "for data_id in range(1, n + 1):\n",
    "    led_file = \"../data/dataset/led/led_{}.csv\".format(data_id)\n",
    "    lcd_file = \"../data/dataset/lcd/lcd_{}.mp4\".format(data_id)\n",
    "    spec_file = \"../data/dataset/spc/spec_{}.csv\".format(data_id)\n",
    "    led = np.loadtxt(led_file, dtype=np.int32)\n",
    "    spec = np.loadtxt(spec_file, dtype=np.float32)\n",
    "    lcd = cv2.VideoCapture(lcd_file)\n",
    "    frame_len = int(lcd.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    lcd_shape = (int(lcd.get(cv2.CAP_PROP_FRAME_WIDTH)), int(lcd.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    lcd_vec = np.zeros((frame_len, lcd_shape[1], lcd_shape[0], 3), dtype=np.uint8)\n",
    "    for frame_idx in range(frame_len):\n",
    "        lcd.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        e, frame = lcd.read()\n",
    "        if not e:\n",
    "            print(\"err\")\n",
    "        lcd_vec[frame_idx, :, :, :] = frame\n",
    "\n",
    "    lcd_data[data_id - 1, 0:frame_len, :, :, :] = torch.tensor(lcd_vec)\n",
    "    spec_data[data_id - 1, 0:frame_len:, :] = torch.tensor(spec.T)\n",
    "    led_data[data_id - 1, 0:frame_len, :] = torch.tensor(led.T)\n",
    "\n",
    "    for data_id in range(n):\n",
    "        for sample_id in range(n_sample):\n",
    "            outer_idx = n_sample * data_id + sample_id\n",
    "            lcd_dataset[outer_idx] = lcd_data[data_id, sample_id:sample_id + n_time, :, :, :]\n",
    "            led_dataset[outer_idx] = led_data[data_id, sample_id:sample_id + n_time - 1, :]\n",
    "            spec_dataset[outer_idx] = spec_data[data_id, sample_id:sample_id + n_time, :]\n",
    "            led_correct[outer_idx] = led_data[data_id, sample_id + n_time, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 267])\n",
      "torch.Size([1, 583])\n",
      "torch.Size([1, 133])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "c1 = nn.Conv1d(n_time - 1, 1, 3, 2)\n",
    "c11 = nn.Conv1d(n_time, 1, 3, 1)\n",
    "c2 = nn.Conv3d(n_time, 1, (3, 3, 1), (2, 2, 1)) \n",
    "c3 = nn.Conv3d(1, 1, (3, 3, 1), (2, 2, 1))\n",
    "c4 = nn.Conv3d(1, 1, (3, 3, 1), (2, 2, 1))\n",
    "y0 = c1(led_dataset[3:4])\n",
    "print(led_dataset[3:4].shape)\n",
    "y1 = c11(spec_dataset[3:4])\n",
    "y2 = c2(lcd_dataset[10:11])\n",
    "y3 = c3(y2)\n",
    "y4 = c4(y3)\n",
    "y5 = c4(y4)\n",
    "y6 = c4(y5)\n",
    "y7 = torch.reshape(y0, y0.shape[:1] + (-1,))\n",
    "y8 = torch.reshape(y1, y1.shape[:1] + (-1,))\n",
    "y9 = torch.reshape(y6, y6.shape[:1] + (-1,))\n",
    "y10 = torch.cat([y7, y8, y9], 1)\n",
    "print(y10.shape)\n",
    "lstm = nn.LSTM(input_size=583, hidden_size=64, batch_first=True)\n",
    "y11, h = lstm(y10)\n",
    "y12 = nn.Linear(64, 133)(y11)\n",
    "print(y12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "dataset = TensorDataset(lcd_dataset, spec_dataset, led_dataset, led_correct)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (spec_conv): Conv1d(10, 10, kernel_size=(5,), stride=(1,))\n",
      "  (c1_1): Conv1d(9, 1, kernel_size=(3,), stride=(2,))\n",
      "  (c1_2): Conv1d(10, 1, kernel_size=(3,), stride=(1,))\n",
      "  (c2): Conv3d(10, 4, kernel_size=(3, 3, 1), stride=(2, 2, 1))\n",
      "  (c3): Conv3d(4, 1, kernel_size=(3, 3, 1), stride=(2, 2, 1))\n",
      "  (c4): Conv3d(1, 1, kernel_size=(3, 3, 1), stride=(2, 2, 1))\n",
      "  (lstm): LSTM(450, 64, batch_first=True)\n",
      "  (lin): Linear(in_features=64, out_features=267, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.spec_conv = nn.Conv1d(\n",
    "            n_time, n_time, 5\n",
    "        )\n",
    "        self.c1_1 = nn.Conv1d(n_time - 1, 1, 3, 2)\n",
    "        self.c1_2 = nn.Conv1d(n_time, 1, 3, 1)\n",
    "        self.c2 = nn.Conv3d(n_time, 4, (3, 3, 1), (2, 2, 1)) \n",
    "        self.c3 = nn.Conv3d(4, 1, (3, 3, 1), (2, 2, 1))\n",
    "        self.c4 = nn.Conv3d(1, 1, (3, 3, 1), (2, 2, 1))\n",
    "        #self.lstm = nn.LSTM(input_size=583, hidden_size=64, batch_first=True)\n",
    "        self.lstm = nn.LSTM(input_size=450, hidden_size=64, batch_first=True)\n",
    "        self.lin = nn.Linear(64, 267)\n",
    "    \n",
    "    def forward(self, lcd, spec, led):\n",
    "        cled = self.c1_1(led)\n",
    "        cspec = self.c1_2(spec)\n",
    "        clcd1 = self.c2(lcd)\n",
    "        clcd2 = self.c3(clcd1)\n",
    "        clcd3 = self.c4(clcd2)\n",
    "        clcd4 = self.c4(clcd3)\n",
    "        clcd5 = self.c4(clcd4)\n",
    "        ylcd = torch.reshape(clcd5, clcd5.shape[:1] + (-1,))\n",
    "        yspec = torch.reshape(cspec, cspec.shape[:1] + (-1,))\n",
    "        yled = torch.reshape(cled, cled.shape[:1] + (-1,))\n",
    "        y0 = torch.cat([ylcd, yspec], 1)\n",
    "        y_rnn, h = self.lstm(y0, None)\n",
    "        y = self.lin(y_rnn)\n",
    "        return y\n",
    "\n",
    "net = Net()\n",
    "net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:\t0\tloss_train=10588.3603515625\n",
      "epoch:\t1\tloss_train=30888.75390625\n",
      "epoch:\t2\tloss_train=28356.974609375\n",
      "epoch:\t3\tloss_train=23084.322265625\n",
      "epoch:\t4\tloss_train=22521.61328125\n",
      "epoch:\t5\tloss_train=32280.126953125\n",
      "epoch:\t6\tloss_train=25479.865234375\n",
      "epoch:\t7\tloss_train=23459.197265625\n",
      "epoch:\t8\tloss_train=18486.75\n",
      "epoch:\t9\tloss_train=24875.603515625\n",
      "epoch:\t10\tloss_train=17164.966796875\n",
      "epoch:\t11\tloss_train=26221.974609375\n",
      "epoch:\t12\tloss_train=17962.966796875\n",
      "epoch:\t13\tloss_train=10797.3125\n",
      "epoch:\t14\tloss_train=14399.306640625\n",
      "epoch:\t15\tloss_train=11959.189453125\n",
      "epoch:\t16\tloss_train=21315.853515625\n",
      "epoch:\t17\tloss_train=17818.3515625\n",
      "epoch:\t18\tloss_train=13150.408203125\n",
      "epoch:\t19\tloss_train=13408.3095703125\n",
      "epoch:\t20\tloss_train=12589.505859375\n",
      "epoch:\t21\tloss_train=10134.68359375\n",
      "epoch:\t22\tloss_train=8010.82568359375\n",
      "epoch:\t23\tloss_train=13094.6953125\n",
      "epoch:\t24\tloss_train=13454.189453125\n",
      "epoch:\t25\tloss_train=13714.3369140625\n",
      "epoch:\t26\tloss_train=13057.23828125\n",
      "epoch:\t27\tloss_train=14578.1357421875\n",
      "epoch:\t28\tloss_train=14525.61328125\n",
      "epoch:\t29\tloss_train=16505.412109375\n",
      "epoch:\t30\tloss_train=15218.5068359375\n",
      "epoch:\t31\tloss_train=11924.7919921875\n",
      "epoch:\t32\tloss_train=12018.458984375\n",
      "epoch:\t33\tloss_train=12912.4326171875\n",
      "epoch:\t34\tloss_train=14038.7041015625\n",
      "epoch:\t35\tloss_train=15411.876953125\n",
      "epoch:\t36\tloss_train=9111.0654296875\n",
      "epoch:\t37\tloss_train=9935.00390625\n",
      "epoch:\t38\tloss_train=10671.7666015625\n",
      "epoch:\t39\tloss_train=14185.037109375\n",
      "epoch:\t40\tloss_train=7547.49169921875\n",
      "epoch:\t41\tloss_train=8896.16015625\n",
      "epoch:\t42\tloss_train=9342.7744140625\n",
      "epoch:\t43\tloss_train=10076.1181640625\n",
      "epoch:\t44\tloss_train=11016.67578125\n",
      "epoch:\t45\tloss_train=12869.1162109375\n",
      "epoch:\t46\tloss_train=9621.630859375\n",
      "epoch:\t47\tloss_train=7777.8017578125\n",
      "epoch:\t48\tloss_train=10079.92578125\n",
      "epoch:\t49\tloss_train=10202.0283203125\n",
      "epoch:\t50\tloss_train=6207.13671875\n",
      "epoch:\t51\tloss_train=7715.10791015625\n",
      "epoch:\t52\tloss_train=8657.0205078125\n",
      "epoch:\t53\tloss_train=8654.6103515625\n",
      "epoch:\t54\tloss_train=6564.193359375\n",
      "epoch:\t55\tloss_train=8978.6064453125\n",
      "epoch:\t56\tloss_train=5909.6435546875\n",
      "epoch:\t57\tloss_train=9843.69140625\n",
      "epoch:\t58\tloss_train=5857.85693359375\n",
      "epoch:\t59\tloss_train=6030.89892578125\n",
      "epoch:\t60\tloss_train=11963.1318359375\n",
      "epoch:\t61\tloss_train=9671.2421875\n",
      "epoch:\t62\tloss_train=9855.86328125\n",
      "epoch:\t63\tloss_train=12148.287109375\n",
      "epoch:\t64\tloss_train=6254.25830078125\n",
      "epoch:\t65\tloss_train=10102.7666015625\n",
      "epoch:\t66\tloss_train=6971.59912109375\n",
      "epoch:\t67\tloss_train=9668.1748046875\n",
      "epoch:\t68\tloss_train=6484.90625\n",
      "epoch:\t69\tloss_train=6526.0068359375\n",
      "epoch:\t70\tloss_train=2877.01513671875\n",
      "epoch:\t71\tloss_train=5920.74609375\n",
      "epoch:\t72\tloss_train=5144.10693359375\n",
      "epoch:\t73\tloss_train=6349.38427734375\n",
      "epoch:\t74\tloss_train=5597.279296875\n",
      "epoch:\t75\tloss_train=2234.973876953125\n",
      "epoch:\t76\tloss_train=6791.9775390625\n",
      "epoch:\t77\tloss_train=5037.8466796875\n",
      "epoch:\t78\tloss_train=7199.01220703125\n",
      "epoch:\t79\tloss_train=1878.852783203125\n",
      "epoch:\t80\tloss_train=9253.2509765625\n",
      "epoch:\t81\tloss_train=9021.58203125\n",
      "epoch:\t82\tloss_train=3374.1796875\n",
      "epoch:\t83\tloss_train=6899.9765625\n",
      "epoch:\t84\tloss_train=5549.4775390625\n",
      "epoch:\t85\tloss_train=3895.02685546875\n",
      "epoch:\t86\tloss_train=5212.05419921875\n",
      "epoch:\t87\tloss_train=4773.2333984375\n",
      "epoch:\t88\tloss_train=5616.8154296875\n",
      "epoch:\t89\tloss_train=2765.767333984375\n",
      "epoch:\t90\tloss_train=8663.1484375\n",
      "epoch:\t91\tloss_train=3044.690673828125\n",
      "epoch:\t92\tloss_train=6726.26904296875\n",
      "epoch:\t93\tloss_train=7567.73193359375\n",
      "epoch:\t94\tloss_train=3476.935791015625\n",
      "epoch:\t95\tloss_train=4658.2685546875\n",
      "epoch:\t96\tloss_train=6584.78857421875\n",
      "epoch:\t97\tloss_train=9613.7080078125\n",
      "epoch:\t98\tloss_train=5122.779296875\n",
      "epoch:\t99\tloss_train=7457.74658203125\n",
      "epoch:\t100\tloss_train=5913.0341796875\n",
      "epoch:\t101\tloss_train=3866.68310546875\n",
      "epoch:\t102\tloss_train=6825.11328125\n",
      "epoch:\t103\tloss_train=3499.66064453125\n",
      "epoch:\t104\tloss_train=8168.134765625\n",
      "epoch:\t105\tloss_train=8099.10693359375\n",
      "epoch:\t106\tloss_train=1850.2159423828125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, (lcd, spec, led, t) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[1;32m---> 12\u001b[0m     lcd, spec, led, t \u001b[38;5;241m=\u001b[39m \u001b[43mlcd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, spec\u001b[38;5;241m.\u001b[39mcuda(), led\u001b[38;5;241m.\u001b[39mcuda(), t\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     13\u001b[0m     y \u001b[38;5;241m=\u001b[39m net(lcd, spec, led)\n\u001b[0;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fnc(y, t)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_fnc = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "record_loss_train = []\n",
    "epochs = 200\n",
    "for i in range(epochs):\n",
    "    net.train()\n",
    "    loss_train = 0\n",
    "    for j, (lcd, spec, led, t) in enumerate(loader):\n",
    "        lcd, spec, led, t = lcd.cuda(), spec.cuda(), led.cuda(), t.cuda()\n",
    "        y = net(lcd, spec, led)\n",
    "        loss = loss_fnc(y, t)\n",
    "        loss_train += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_train /= j + 1\n",
    "    record_loss_train.append(loss_train)\n",
    "    print(\"epoch:\\t{}\\tloss_train={}\".format(i, loss))\n",
    "\n",
    "    # if i % 10 == 0 or i == epochs - 1:\n",
    "    #     net.eval()\n",
    "    #     print(\"Epoch: \", i, \"Loss_Train: \", loss_train)\n",
    "    #     predicted = list(input_data[0].view(-1))\n",
    "    #     for i in range(n_sample):\n",
    "    #         x = torch.tensor(predicted[-n_time:])\n",
    "    #         x = x.view(1, n_time, 1)\n",
    "    #         y = net(x)\n",
    "    #         predicted.append(y[0].item())\n",
    "    #     plt.plot(range(len(sin_y)), sin_y, label=\"Correct\")\n",
    "    #     plt.plot(range(len(predicted)), predicted, label=\"Predicted\")\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[149.0563, 159.1218, 158.3785,  ..., 154.5041, 152.7531, 154.5618],\n",
      "        [182.0887, 194.9094, 193.6705,  ..., 190.0205, 187.9965, 190.1384],\n",
      "        [157.8981, 170.3632, 168.8365,  ..., 164.9752, 163.2004, 164.9760],\n",
      "        ...,\n",
      "        [  9.5372,   9.6891,   9.7086,  ...,  10.2484,  10.1610,  10.3994],\n",
      "        [  9.5372,   9.6891,   9.7086,  ...,  10.2484,  10.1610,  10.3994],\n",
      "        [  9.5372,   9.6891,   9.7086,  ...,  10.2484,  10.1610,  10.3994]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data_id = 7\n",
    "led_file = \"../data/dataset/led/led_{}.csv\".format(data_id)\n",
    "lcd_file = \"../data/dataset/lcd/lcd_{}.mp4\".format(data_id)\n",
    "spec_file = \"../data/dataset/spc/spec_{}.csv\".format(data_id)\n",
    "led = np.loadtxt(led_file, dtype=np.int32)\n",
    "spec = np.loadtxt(spec_file, dtype=np.float32)\n",
    "lcd_vid = cv2.VideoCapture(lcd_file)\n",
    "\n",
    "n_time = 10\n",
    "frame_max = 150\n",
    "n_sample = frame_max - n_time\n",
    "frame_len = int(lcd_vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "lcd_shape = (int(lcd_vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(lcd_vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "lcd_vec = np.zeros((frame_max, lcd_shape[1], lcd_shape[0], 3), dtype=np.uint8)\n",
    "for frame_idx in range(frame_len):\n",
    "    lcd_vid.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    e, frame = lcd_vid.read()\n",
    "    if not e:\n",
    "        print(\"err\")\n",
    "    lcd_vec[frame_idx, :, :, :] = frame\n",
    "\n",
    "lcd_vec = torch.tensor(lcd_vec)\n",
    "led = torch.tensor(led)\n",
    "spec = torch.tensor(spec)\n",
    "lcd_test = torch.zeros((n_sample, n_time, lcd_shape[1], lcd_shape[0], 3))\n",
    "spec_test = torch.zeros((n_sample, n_time, spec.shape[0]))\n",
    "led_test = torch.zeros((n_sample, n_time - 1, led.shape[0]))\n",
    "led_test_correct = torch.zeros(n_sample, led.shape[0])\n",
    "for i in range(frame_len - n_time):\n",
    "    lcd_test[i] = lcd_vec[i:i + n_time, :, :, :]\n",
    "    led_test[i] = led.T[i:i + n_time - 1, :]\n",
    "    spec_test[i] = spec.T[i:i + n_time, :]\n",
    "    led_test_correct[i] = led.T[i + n_time, :]\n",
    "lcd_test = lcd_test.cuda()\n",
    "led_test = led_test.cuda()\n",
    "spec_test = spec_test.cuda()\n",
    "led_test_correct = led_test_correct.cuda()\n",
    "net.eval()\n",
    "y = net(lcd_test, spec_test, led_test)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"output.csv\", y.cpu().detach().numpy(), fmt=\"%d\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
